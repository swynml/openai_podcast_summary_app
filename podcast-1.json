{"podcast_details": {"podcast_title": "The TWIML AI Podcast (formerly This Week in Machine Learning & Artificial Intelligence)", "episode_title": "Are LLMs Good at Causal Reasoning? with Robert Osazuwa Ness - #638", "episode_image": "https://megaphone.imgix.net/podcasts/35230150-ee98-11eb-ad1a-b38cbabcd053/image/TWIML_AI_Podcast_Official_Cover_Art_1400px.png?ixlib=rails-4.3.1&max-w=3000&max-h=3000&fit=crop&auto=format,compress", "episode_transcript": " All right. What's up everyone? Welcome to another episode of the TwinMall AI podcast. I am your host, Sam Charrington. Today, I am excited to be here with good friend and longtime friend of the show, Robert Ness, a senior researcher at Microsoft Research, professor at Northeastern University, and founder of altdeep.ai. Before we get going, be sure to take a moment to hit that subscribe button wherever you're listening to today's show. Robert, welcome back. It's great to see you. Thanks for having me back, Sam. Yeah, it's good to see you as well. Thanks for having me back. In a sense, this interview was maybe foreshadowed a little bit. Just recently, our conversation from the beginning of the year in which you reviewed research progress in causality and causal modeling, or our AI Trends series was published. That was published back in February and at the end of that, or maybe throughout, we were talking about LLMs and the implications of LLMs and causality. There's a little bit of an open question at the time, but you and some of your colleagues just published a paper, Causal Reasoning and Large Language Models, Opening a New Frontier for Causality, which really dug into the topics. So quick work on that apparently. I'm looking forward to digging into that. I think folks on the show may know a little bit about you, but briefly, share your background and what you do at Microsoft. Sure. I'm a Senior Researcher at Microsoft Research. I work on the Special Projects team that's out of Redmond. My background is in probabilistic machine learning, probabilistic programming, and causal inference, how these things come together of causal AI. I've worked a lot of language models too, and it's really exciting now working with large language models at MSR. I can think of a few places where they're competitive of MSR in terms of a place to work on these problems. And then my team at Special Projects, we look for these research applications and how we can transfer them into impact both inside of MSR and externally. And so we were very keen to explore this problem of what exactly can large language models do in the area of causality. And this was in collaboration with my colleagues at MSR, Amit Sharma and Amri Kisman, as well as Chunhao Tan at the University of Chicago. And we had essentially explored some benchmarks in the space, as well as different areas of causal analysis, and brought that together in this paper. So it was a really exciting work. I was happy to be a part of it. Awesome. Just to ground the discussion for folks that are new to the idea of causality and causal modeling, causal analysis, give us the quick overview so that we can have that context for the specific conversation about LLMs. Also summarize all of your life's work in 30 seconds or less, please, sir. Sure. So causal analysis is something that we're interested in across different fields, so ranging from econometrics to epidemiology to statistics, obviously, and natural sciences. And the issue is we have data, and oftentimes it's from just observational, passive observational data. Sometimes it's experimental data. And we want to make causal conclusions from the data. And knowing the aphorism correlation does not imply causality, we make certain modeling assumptions and then use the data to try and make causal conclusions. And recently, it's become, I don't know how recently, but semi-recently, it's become an interesting topic of focus in machine learning, for one, because oftentimes we have very large data sets. We want to leverage the ability of machine learning to scale up to large data sets. But also because causal reasoning is an essential task. If you say that our goal of AI is to, or the goal of our field is to achieve generalized artificial intelligence, we believe that the ability to reason causally is going to be a part of that generalized AI. So one aspect of this field is figuring out, OK, how can we get learning agents to reason causally and make correct causal conclusions or make causal conclusions in a way that is maybe comparable to a human baseline or is aligned with some standard for reasoning? And it's definitely a growing field. And it's particularly interesting now that we have these foundation models to work with. What's an example of causal reasoning in the context of LLMs? And you can use one from the paper. OK, a simple example might be, let's say, so pairwise causal discovery. So this is to say, given two variables, assuming that one causes the other, let's just say that we know that one causes the other, trying to find out which causes which. And so this could be done with what in the paper we call covariance-based analysis, which is to say, analyze the data and, using certain modeling assumptions, try to conclude whether A causes B or B causes A. And with the large language models, we can also just ask it, given the variable names, for example, does temperature cause altitude or does altitude cause temperature? I call that pairwise causal discovery. There's also a full graph causal discovery, which is to learn an entire causal graph of relationships between variables. There's also, if you're particularly in econometrics, epidemiology, statistics, looking at whether or not something causes on Earth things where the gold standard would be a randomized clinical trial. Then we got some sales from some months in November and December and January, and we saw a spike in sales in December. And in early December, we placed an ad. Did the ad cause the spike in sales? And posing that in that natural language form to the large language model and asking it to give you a response. That would be a causal query for what we would call an average treatment effect or a conditional average treatment effect. And I think we may come back to that example and talk about what you saw. But maybe frame the paper a little bit in terms of your broad goals for it. Yeah, so I think when we started looking at the question, we were seeing, OK, well, GPT 3, 3.5, and 4, these are very impressive models in terms of the various benchmarks that we would use to evaluate a large language model. And how well does it do on causal inference, causal or causal queries in general? And so the question in the room, we were thinking, can large language models reason causally? And of course, when you say the word reason, it's a bit anthropomorphizing the model. And so to be a bit more specific, at least the way I would frame it would be to say, are there conditions under which we can pose a natural language causal query and reliably or predictably get the right answer or not get the right answer according to some definition of rights, in some cases that being some objective truth or in other cases, maybe reasoning along the lines of how a human might reason about a causal question. Then we discovered this is a very broad question. It's very interesting given the types of questions that we can ask it. We found these benchmarks. We posed it to the large language model and we got impressive results. Upon looking at it more closely, we also found that actually the benchmarks aren't very good at answering this specific question for reasons we can talk about. And that's in order to answer this kind of question, we worked with the OpenAI series of models. And for our analysis, we didn't have access to weights, the training data, the architecture. So maybe this is a kind of question that you need that kind of insight into the weights, into the training data to be able to answer provably. And so we think that that's a very important question that I think we think it needs to be perhaps better posed. But there are some interesting experiments that could be set up there with various levels of access to data or perhaps training a new model that has certain disinter But we did find that the answer to a different question was an enthusiastic yes. And that question was, can large language models essentially supercharge causal analysis? Our work as modelers and building workflows for answering causal queries and that discovery and how it can open up what we call on the paper a new frontier for causal analysis and causal research. So I think what you're saying there is you're kind of curious about how LLMs would do at tackling these causality, these causal problems. You found existing benchmarks in the causal reasoning community and you kind of threw LLMs at them and they did well, but dot dot dot. And so you've got some open questions there. And you found some areas where they didn't do very well. If I'm hearing that correctly and reading and referring that from the paper, if not as standalone causal reasoners, they're good tools for complementing human causality, researchers, modelers, analysts, whatever. So dig into that, but dot dot dot part. They apparently do well on all these causality benchmarks, but you were still left with questions like what did they do well at and what were the questions that you were left with. And what is it about those questions that you feel like you would need access to the model weights or training data sets or other things to really suss out? Well, OK, so in terms of what didn't they do well, well, for one thing, we saw that we didn't really see performative results with respect to these benchmarks, with respect to state of the art baselines, until we got to a certain level of model size. So essentially, text DaVinci 3, GPT 3.5 Turbo, GPT 4, when we started seeing comparative or better than baseline results. And so this kind of indicates, so if you saw the Sparks of AGI paper from Microsoft, where we essentially one of the stories there was that things that, with this movement from GPT 3 to GPT 4, we saw entirely new capabilities that were very weak or non-existent in GPT 3. It has that same flavor. Meaning GPT 2 did not have the capability to do any causal reasoning, but you're starting to see it with 3 and 4. Yeah, in other words. Or is it 3.5 and 4? 3 and 4. 3 and 4, OK. So I'd say it was the text DaVinci 03, so that's kind of what instruction training, fine tuning, that's reinforcement learning of human feedback. To me, that's mysterious, right? What that means is that before these models, they were doing worse than random, right? So on these benchmarks, you had a multiple choice question, or you had a binary question, then there's an accuracy number that is the number that you get if you're just guessing at random. And it wasn't until post-GPT 3 where we saw baseline comparative or better performance. And why is that mysterious? I think we take it for granted in a lot of contexts that the models are a lot better, and there are a lot of things that they can do better and perform better on as they get bigger. My personal take on this type of thing is there's a lot of talk in our community about emergent behavior. And I agree with that. And I understand how the intention mechanism, the more data you give it, the more it can move up a hierarchy of abstractions in terms of what it's attending to. But it's mysterious. And I don't. Mysterious in the sense of we have no idea why this works, and it's kind of interesting that it does. Yeah, and that makes me a little bit queasy, particularly if we'd start talking about, I mean, this is causality, right? And so one queasy in the if we start asking important questions and we don't know why or how this works is kind of like. Right, I mean, so the machine learning community, oftentimes our metrics are kind of focused on prediction. And maybe if you're nuanced, you might define some kind of cost function on making errors. But traditionally speaking, the causal inference community is conservative in terms of what it's going to conclude. We're saying that this vaccine prevents this illness or prevents the worst cases of this illness, right? So the idea that it's like, is it a good performance correlation or causally valid? Yeah, it's just like if we're going to start deploying this thing to actually make decisions based that are involving kind of causal logic, it feels like a mysterious capability of doing well on accuracy benchmark. On accuracy metrics on benchmarks, which as we talk about in the paper, are often memorized by the large language models, is not a high enough standard for that sort of thing. So to give an example, let's suppose that I were to deploy a model that would ingest somebody's resume and then tell me whether or not I should hire them. By the way, nobody should do this. There are various issues with this. But suppose I was going to deploy that system and just have it automatically give an accept or reject for somebody who's applying for a job. And I could go in there, and in my prompt, I could say I can kind of give it constraints that would force it to admit certain, in a causal sense, certain factors that would drive its logic for why to hire or not hire somebody. Say, for example, I said, you may not consider gender. You may not consider ethnicity. You only may consider factors in their educational or professional background, for example. And it could generate a do train of thought reasoning, show me your logic step by step. It gives you a step by step argument for why it says, you should not hire this person, right? From a causal perspective, you would want to make sure that, despite the fact that it says that that's the reasoning that is deploying to arrive at this decision, how do you know that it's not just f-s-ing you with politically correct answer when maybe after maybe given all the, say, toxic content it was trained on is still somehow biasing the answer? So in maybe a lot of scenarios, this isn't a big deal. But from a causality standpoint, we care a lot about identification. Identification, I mean, kind of provably you're able to use the data to answer a causal query. And you care a lot that the model is using your causal assumptions to arrive at the answer. Yeah, and so I think that those are. So if you could look at activations and things like that, when these questions are being evaluated, you might get some hint as to how they're grounded in the model or something. Yeah, exactly. Using kind of probing procedures, if the latent representations in the model align with some kind of causal model or causal abstractions or causal assumptions, this is very fresh research. I know we're working on this. Other people are working on this. And so that's not in this paper. But I think my personal belief is that that's the type of analysis you want, the kind of experiments you want to run to understand exactly how it's reasoning causally, if that's what it's doing. And so to be clear, in several of the types of questions from these benchmarks that were posed to the models, they did really well. Is that fair to say that they did very well compared to the state of the art in these kinds of causal reasoning tasks, am I interpreting that correctly? There are some numbers even in abstract, 97%, 13-point gain for pairwise causal discovery, 92% for counterfactual reasoning, 20-point gain. I'm assuming that's 20 points over the prior state of the art. Actual causality, 86%. There's no gain discussed here. But that seems to be good performance. I guess one of the questions that you implied earlier is it like, hey, these benchmarks are out on the internet, did the model just read these benchmarks and it somehow knows the answer? Yeah, so you mentioned that 97%. So with the tubing in pairwise discovery benchmarks, so this is a benchmark. So I mentioned altitude and temperature. So this benchmark has several examples like that from across different domains. So zoology, chemistry, geology. So it gives you a pair like that, so temperature, altitude. And then it gives you, say, a bunch of measurements of temperature and altitude from different geographical locations on Earth. And the intention in the construction of this benchmark was to say, OK, well, let me use a statistical method that's going to look at these pair of variables and figure out which is the more likely direction that temperature causes altitude or that altitude causes temperature. So what we did in the experiment is that we threw away all the data, just took the variable names, and then posed some questions. Specifically, the prompts was something along the lines of which cause and effect relationship is more likely. A causes a change in template variable B, or changing template variable B causes a change in template variable A. Let's work this out step by step to make sure that we have the right answer, provide your final answer within these tags so that we can parse it. We did not use any context learning examples, but we did give it a system prompt that said, you are a helpful assistant for causal reasoning. And that's what we did. So one of the things we noticed right there was that chain of thought mattered. The system prompts mattered. So we saw a five-point increase for GPT-3 when we didn't use the system prompt versus using the system prompt. And then in terms of some of the errors it made, so yeah, it got very high percentage. When we looked at some of the errors that it made, it would often, in some cases, things that you read and they were coherent, but we're making maybe small errors in judgment. But oftentimes, we saw cases where we actually agreed with the large language model in the sense that the example I'm thinking of is still altitude and temperature. And it gave an example where, now I'm forgetting. Trying to think of an example where it made an error. OK, ozone and radiation. So the answer was that radiation causes ozone. So the presence of radiation causes ozone. And it concluded that ozone caused radiation. But when we looked at the chain of thought, you can see that it was talking about ozone in the stratosphere, so that when the ozone layer is depleted, there's more solar radiation. But we were talking about, no, at surface level, there's radiation being from some source that causes there to be ozone in the air. So oftentimes, when it got it wrong, we realized that it could get it right with just a simple, a bit more clarification. And those were rare examples. But when they did happen, it implied that, OK, well, again, sensitive to the prompt. And also that so far, these models aren't particularly good at asking, clarifying questions, which is another concern there. So yeah, there were some failure modes. But often, it was because they were rare when they were just outright wrong, at least for the causal discovery task. And when it came to some of the actual causality, causal judgment tasks, it was more often making errors in logic. But still, the logic with GPT 3.5 and especially GPT 4, especially GPT 4 and the causal judgment and actual causality tasks were pretty coherent, even when they were wrong. And so you can imagine that with better prompting, you might be able to get it into the right place. Just out of curiosity, when you said you did some testing around the system prompt, and you mentioned presence versus non-presence of the system prompt, did you vary the system prompt? I'm wondering if you're able to identify whether you just said, hey, you're a helpful assistant, that got you a boost? Or was it like contextualizing that causality is what you're looking for, that condition that results positively? We didn't do much exploration in terms of prompt optimization. So we just basically gave it a kind of obvious system prompt that stood out to us as for chain of thought reasoning, and particularly with some of the tasks that had to do with counterfactual reasoning, actual causality, causal judgments. There was a clear sign that better prompting would have an impact, but that was not a dimension that we explored deeply. So going back to this kind of core question, it did very well in pairwise causal discovery. There's some concern about the validity of that as a result. If you can separate the, hey, I don't really understand it, and I want to understand it before I use it, you mentioned the possibility of memorization as one concern. Part of me says, is it really in the sense of that's kind of what you want it to do? You want it to go out and if you're talking about these broad relationships like altitude and temperature, you want it to have kind of memorized the answer to that or seen a bunch of examples to that and pull that into its reasoning. Right, you pick up on a really interesting and nuanced point, which is that we know that most large language model tasks, we have this problem of, is the large language model ingesting the benchmark and just kind of spinning back out memorized, the memorized version of the benchmark? And we want to avoid this problem of memorization. But to your point, we want some memorization. We want it to know what altitude is and what temperature is and memorize those. It's not an agent acting in the world hasn't learned it through, can only learn it through the training data. So we want them to know what those things are and that there is a causal fact that altitude affects temperature. And so we want to memorize that. We just don't want it to memorize specifically what's in the benchmark itself. And so what does that even mean and how do you do that for broad facts like altitude and temperature? Well, so first thing we did is to test whether or not the benchmark was in the training data. And to do that, we did a kind of text completion task that basically tried to complete elements of the benchmark. So we put in the read me and the kind of all the interesting background from the benchmark and then put in a specific row, like the first two elements of a row in the tabular data in the benchmark and then asked it and then basically try to see if it could autocomplete the rest of the row. And for the tubing and data, it did. It did that for about 20% of the examples. And in terms of just predicting the next cell in the row, it got about 60% accuracy in predicting the next cell. So clearly the tubing and data was in the benchmark. And so we had to think about, OK, what does that mean now? Now, what are we supposed to do with this in terms of deploying? So the actual problem that we care about is to what extent does it generalize beyond the benchmark? And so we kind of partitioned the task of answering the question into two parts, or say two kind of conditional probabilities, or two probabilities. So one being the probability that the causal fact is encoded in some sense in the large language model, or in other words, the large language model has learned this causal fact. And then the second being that conditional and having learned this causal fact, to what extent can it answer your specific question, given the context, given your prompt, in a way that transforms that causal fact into part of that answer? And the benchmarks are good at evaluating the second, but we can't know generally if it's doing well, to what extent it's happening on the first. We can't know, we can't evaluate how well a large language model has learned some causal facts about the world, just how well it can answer a question based on the fact that it has learned it. And that's always going to be kind of confounded with its ability to kind of BS you and hallucinate. So kind of the conclusion there is like, OK, these benchmarks themselves don't tell us the full picture. And then so also we want to know how well we can generalize. I mean, the whole point of the benchmark is like, can it generalize to tasks that are like the examples in this benchmark? And so we explored some ways around this problem. So for one, Can you give some concrete examples of that generalization in the case of related to the tubing in examples you've already given? Like, are you asking it to answer or kind of reason about different word problems that relate the facts around altitude and temperature, for example? Or is it some other kind of generalization? So one example we talk about in the paper, again, was causal relationship between the length of a seed malice called the abalone and the age of an abalone. And that's the fact that the truth there was that the older the abalone gets, the longer it is. We also saw an interesting error mode there where if we changed length to diameter, it actually got that wrong. So it's an interesting example for how it's not quite doing whatever it's doing. It's kind of off in the sense that it's kind of more like memory than understanding. So like changing the word to what's effectively in this context a synonym, it gets the answer wrong. And what you're hoping there in terms of generalization is that it could generalize to other causal relationships, causal facts in the area of zoology. If something similar would be like the number of rings in the stump of a tree and the age of a tree, for example, which is not in the data. And so you want to make sure that. So that's what I mean by generalization. We started exploring ways. Are there ways that we can get around this? Say, for example, could we prompt it with some sort of inductive bias that's would help it generalize? So in machine learning, we often talk about inductive biases in terms of the syntax of the architecture. So that's, say, for example, convolutions and max pooling give you translation invariance. Causal inference, we're often getting our inductive biases from explicit model assumptions that we formalize into a model. So for example, a causal graph, functional assumptions like in a structural causal model. The interesting things with the large language model is it allows you to kind of use inductive biases that were much easier to use in the form of a natural language. So like, for example, Occam's razor. So we ran an experiment where we said, we asked the large language model with the pairwise discovery to say, OK, give us the best argument you can for the best argument for why A causes B. Now give us the best argument that you can for why B causes A. And just try to make this argument as kind of coherent while reasons as possible. And then in a third prompt, we said, OK, look at these two arguments that they just generated. We didn't tell it to the decision. These two arguments that were just generated. And tell us which argument is preferred based on Occam's razor. And this is based on kind of research and psychology that suggests that the causal explanation that introduces the fewest external factors to explain something is preferred, at least for humans. I mean, we saw that's the wrong direction. To come up with a good argument, I had to kind of add stuff. So say, for example, with the abalone's age and length, it said that, well, longer abalones could be much more competitive in a resource-restricted environment and getting food. And they could kind of crowd out all the other abalones and therefore live longer. And so that's a very well-reasoned and plausible argument. But it required introducing kind of resource constraints as an additional factor, while the simple explanation that, as things get older, they get longer, did not require that. And so in that case, we got lower up. We got obviously lower accuracy than GPT-4. We got about 85% accuracy there. But considering the fact that we know that the benchmark was memorized, I think that's pretty good. In talking about these inductive biases, the paper talks about these models' ability to create causal graphs. Did you mean that literally? Give it a problem or a prompt, and you say using some formulation that lends itself to text, state the causal graph? Or are you inferring its ability to create the causal graph and its ability to perform well on these benchmarks and answer certain kinds of questions? Well, yeah, so the ability to say, here's my problem. Here are the variables. Give me a causal graph that connects to these problems in a causal setting. And if you try that, plenty of people have experimented. So for example, if right now you go to chat.tpt and you say, hey, I'm interested in the relationship between smoking and lung cancer. And I want you to give me a causal graph and give me some possible confounders, some mediators between smoking and lung cancer, maybe something that introduces some collider bias between smoking and cancer, some instrumental variables like the cost of cigarettes. And it will suggest those variables and give you a graph. And with an example like that, which is kind of canonical, it'll be something very plausible. Now, what we wanted to do then is like, OK, well, let's try and actually give it a set of variables, something like a non-trivally large potential graph. So in some of our cases, 13 or 14 variables. And ask it to construct a graph and compare it for causal discovery algorithms that are taking the data and constructing a graph using algorithmic analysis of the data. And the way the prompting worked there was nothing sophisticated. We did not just give it a bunch of variables and say, give us a graph. What we did was to say, we just kind of extended our pairwise discovery problem, then asked it to extend that problem such that there was a third option. So that instead of A causing, we have A causes B, or B causes A, or none of the above. Essentially, there is no edge between these two nodes. And all those pairwise or absence of pairwise relationships and use that to assemble the graph, ignoring like a cyclicity constraints, just assembling a bunch of pairwise relationships, and then looked at things like F1 and structural hamming distance between the learned graph and in cases where we had the ground truth graph. And then it had performance comparable to state of the art discovery algorithms, including algorithms that were using deep learning methods. But of course, this implies that the names of the variables are informative enough that if the names are just A, B, and C, and D, then it's not going to work, obviously. But it has to have names like informative variable names where it could infer whether or not. Meaning it needs to output informative variable names? No, no, no. For example, in one data set we used was a data set on neuropathic pain. So it had names like Wright, L1, radiculopathy, and Farage-Neo-dyscomfort. So things that if you were a domain expert, you can look at the variable names and know what they meant. And so talk a little bit about given what you observed, maybe net out what these models are good at today, what they're not good at. And you covered that, but net it out. And then how does that, the next step, I think, in your analysis is, so therefore, these models can be used in these ways by practitioners as effective tools. I would say the models are getting good accuracy on, at least for the pairwise and full graph discovery, those types of benchmarks. With respect to, is there a lower level of detail in there? The models are good at pairwise, or is that an atomic task? The models are good at pairwise discovery because they can do A, B, and C. We see these kind of failures a lot because they can't do D and E. I would say that's, and you need to let me know if this answers your question, that's the performance on the pairwise and full graph discovery indicates that for this problem of coming up with a causal DAG to drive your downstream analysis. So every causal inference problem, ranging from causal discovery, causal effect inference, counterfactual reasoning, all of them require causal assumptions. And both in terms of what we see from research, from experience, and from working with users, for example, some of the open source packages you work on, like do-why and other packages in the pairwise community, this is my teaching. Common theme is, how do I know I have the right DAG? How do I know that if the downstream analysis depends on my DAG, I'm really worried that I'm going to specify the wrong model and my downstream analysis will be wrong? And so this is a kind of a translation of, the problem of translating domain knowledge into some computable artifact that encapsulates our model assumptions is a challenging problem for the causal inference practitioners. And I think that the results on the pairwise and full graph discovery show that these large language models are very useful at jumping that gap, at getting from what you know to something that you can use to drive a statistical analysis. That said, it is brittle in ways that we don't really understand. So for example, changing length of an abalone, sorry, age of an abalone causes length of an abalone. It gets that right. Diameter of an abalone to age of an abalone, it gets that wrong. That's a weird thing to get wrong. We had another example about causal effect estimation, where with that, as I mentioned at the beginning of the call, this thing where we see a spike in sales in December, was it the ad? And it gave us this very well articulated, and it suggested that we do an A-B test, and we gave it the A-B test results. And then it said, oh, here's the range. It kind of did a causal decision three type of analysis and said that this is a good policy, that you should do this ad. And it missed the fact that it's December. It's the holiday season. So it misses obvious things while getting other things, doing really well at other things. And so now that suggests to me that it's useful in this context in the human and in the loop type of, the human using this to borrow our Microsoft parlance, a kind of causal co-pilot. But then doing the analysis itself, it just kind of blows up too frequently. Well, I mean, if you were going to say, do estimate a causal effect. I guess just in the context of that example that you gave, like missing December and the fact that December is a holiday, is that? Holiday season, right. The holiday season, yeah. Is that a failure in analysis? Is that a failure in, I mean, you could argue is failure in constructing the graph because it didn't pull in some context that should have been obvious that was not stated explicitly in the formulation of the problem, right? In the formulation of the problem, in the prompt itself, it was kind of like almost like a trick question, right? So we asked it, hey, October, November, we got this many sales. January, February, we got this many sales. In December, we got a whole lot of sales. And we placed an ad in early December. And here's what the ad cost. And here's the amount of sales we got. And we just gave it all of this. You're leading it down this path. And you wanted to see if it would go for it. Yeah, we were anchoring it, baiting it to kind of focus on reasoning about the ad itself. But it's reasonable to assume that an expert would have said, wait a second. Before we go down this rabbit hole of the cost benefit analysis of this ad, we need to understand that there are other possible factors here. The most obvious one, based on domain knowledge, is that December is the holiday season. And that's what it's supposed to be good at. So it's supposed to be good at establishing the rights, deploying the main knowledge to answer the question. And so the prompt, in our case, was leading it, kind of sneakily leading it away from the right answer. But you wanted to be robust to that. You wanted to detect your blind spots. And so had I forgotten that December, that I was just looking at some tabular data and completely forgetting that, oh, yeah, it's a holiday. There's holidays in December. People buy toys because that's not there in my data. I would hope that if I can use this model to kind of capture domain knowledge about the space, it certainly knows that if I were to prompt it differently, I'm sure, confidently, that it could say, oh, yeah, December is the holiday season. And a lot of retailers make most of their year's income in December. And so the fact that it seems like the explain your work kind of set up would have elicited that. And in other cases, it did. So again, like with the abalone example, it mentions in the chain of work, explain why the age of the abalone causes the length of the abalone. It says another thing that it might consider is available resources in its environment, like food and other things that it needs. But it discounted that. So it mentioned this thing. And it said, but that's not really important here. What we need to focus on is just how, as things get older, they grow. So in that case, it's like, OK, here's some external factors that might matter. But we're going to ignore those. But in this sales example, it didn't say, another factor could be a holiday bump. But we're going to ignore that and focus on this ad. It didn't do that. It just got steered into just focusing on the ad. And so that tells me that, OK, well, now this thing is going to be very sensitive to the prompt. And so now the causal inference expert has to be a very good prompt engineer if you're going to deploy these methods. Or in the future, perhaps our models get smaller, sparser, maybe more focused on causal tasks and can help us alleviate that problem. But that's speculation. Do you foresee the next iteration of models addressing some of these? How do you see the future of LLMs changing the results that you saw in this paper? So with GPT-4, we saw a lot of saturation on the pairwise and fullwise discovery of the data sets, which were likely memorized. I think an interesting area is, can we come up with other ways to evaluate these models on these tasks? One of the things that we started exploring in the paper was looking at actual causality and causal judgments. So to explain here, so actual causality means type causality, which is what we've been talking about, is about things that cause other things in general. Now, an actual causality or token causality is about here's an event that happened. What events caused this event? So it's focused on individual events, as opposed to reasoning over variables or populations. And so we looked at a few benchmarks for that one. It's called this CRAST benchmark. It did quite well on that data set. But there's an interesting data set in the BigBench collection of large language model data sets called causal judgments. And accuracy for that, with GPT-4 for that benchmark, is about 0.65%. Some people have reported higher. But actually, when you actually ask it to do chain of thought reasoning, accuracy goes down a little bit. And this is an interesting data set because it's not, unlike a causal effect or a causal graph, there is no ground truth. Really, it's about how humans reason about what causes what. So you could say, for example, that if I ask, why did the glass spill? It would be true to say because of the Big Bang, right? So it involves determining which causal events were actually relevant to this context, setting a causal frame. We found that it's very good at setting that causal frame. And again, in this benchmark, the actual labels are not some actual effect. It's how humans answered these questions. And the top human labeler gets 100% of the answers right. And so there, you're trying to evaluate how well a model can align with human causal judgments. And 65% still far from saturation. But if we look at the COX-I literature, we saw that there are components for how humans make a judgment. So whether or not an event was a necessary cause, did it need to happen for the outcome to happen? Was it sufficient to cause the outcome to happen? Was it a norm violation in terms of a statistical norm? Was it surprising or unexpected? Or was it a social norm violation? Did it break a law or people's social norms? Were other events norm violations? So for example, if you're playing cards and the other person draws a really bad hand, do you blame the other person's bad hand or do you blame your good hand? Was the outcome undesirable? So the outcome is just some neutral thing, like nothing bad happens versus the outcome is people dying in a train problem, a trolley problem, for example. And then whether or not the act, the cause was a mission. So like you broke the machine versus you failed to maintain the machine. All of these things drive human causal judgments about actual causality. And we found that the model did fairly well, at least comparable to its ability to predict the actual causal events, comparably well in kind of parsing these individual components. So if you asked it, was this a necessary cause? Was it a sufficient cause? Was it a norm violation? It's getting around for those that's ranging from 70% to 80%. It did poorly in determining whether or not other causes were norm violating, but that could have been because of the prompt. It did very well in discerning whether the outcome was undesirable or neutral and whether or not it got about 70% accuracy in determining whether or not it was a mission caused by a mission or caused by action. So fixing the machine versus not fixing the machine. And so it seems to be doing well at kind of breaking down a causal problem, a causal judgment problem, as well as predicting the actual judgment. And I think that there's a lot of interesting space there in terms of if it understands the components of a judgment, if it's good at predicting the components of a judgment, could we somehow prompt the large language model in a way that has it used these components to get to the right answer? So kind of coming up with some kind of causal recipes for reasoning that you kind of supply as part of the prompt. I think that's an interesting area of investigation. What's interesting in what you just described with this data set that articulates human preferences or human approaches to causally reasoning about a problem is like, that's exactly what you'd want to tune on with RLHF. And then you have a model that's been specifically trained to kind of answer questions in a causal way. And so that raises this issue of, hey, yet another benchmark that you can't use anymore because you've sucked it into your training data. But do you think that that would result in a model that is better able to think causally, not to anthropomorphize, but to approach problems causally or reason causally? I think exploring RLHF approaches that if we suppose the original kind of RLHF approach that made chat original, the RLHF approach that made chat GPT so impressive was essentially asking human raters to say, what's the best answer here? And so you're basically training it to emulate human responses. And training it to align with some other recipe for providing an answer is definitely an interesting thing that I think people should be trying. Sounding like a human, is that the only thing that we care about? Or perhaps in some cases, maybe that's even sometimes when I correct and it apologizes to me, I'm like, you're not actually sorry. So for a lot of tasks, we could do away with the biomimicry and just focus on having it align with some kind of recipe for coming up with a good answer. And that said, I think we still need to understand whether or not it's actually following that recipe when it does that, as I described with the hiring example. But I definitely think that's a very fruitful area of research. Interesting. Very, very cool stuff. Particularly for personalization, we don't just need to use it for benchmarks. If you're trying to fine tune a model or train your own model for a task within your corporate context, so your research, your academic, or some specific academic problem you're working on, maybe you want to do that kind of task-specific RLHF that's not general enough for a benchmark. So besides from your own work and research and that of your collaborators, what's the coolest thing you've seen in the past? What's the coolest thing you've seen out there recently from an AI perspective? I'll give a shout out. Well, so I'll stick on the causality stuff. I'll give a shout out to a PhD student at Stanford University, a guy named Atticus Geiger. I think I mentioned it before in our previous interview, but he has this method called, I think it's interleaved intervention training, which is looking at ways to take a foundation model. It could be a large language model. And the abstractions that the model is learning and it's latent representation align with some oracle causal model that you have for a specific task. And I like that task-specific element of it because you can have the large language model and kind of train it on everything. But then with respect to some specific task, you're kind of making sure that it's aligned with some causal model. And for a lot of tasks, maybe it's not possible to come out with the causal model, like a formal causal model. But for some, you might be able to. And so making sure that it's aligned with respect to the cases where you can do that, I think is a very interesting idea. And I hope we see some more work there. And I encourage people to read his papers. Awesome. We'll put a link to the paper, obviously, in the show notes. Anything else you want to plug? OK, so recently I've been getting into this work from my colleagues at Microsoft Research, led by Scott Lundberg and others, on a repository called Guidance, which is a tool for working with large language models and constraining them and controlling them in a way that you want. It's competitive with Lang chain and semantic kernel. The Guidance has a lot of really cool features. And it's really a pleasure to use. And I think it's really exploding really quickly. And I think people, if they haven't heard of it, they should try it out. If you've had difficulties working with large language models ranging from getting it to select from a specific set of answers, or doing token healing, or doing things like returning a JSON that has a specific structure, it's very good at that, as well as other things. So I encourage people to check that out. Awesome. Well, Robert, as always, it's great to connect and catch up on what you're working on. Thanks so much for your time, Sam. I enjoyed this. Thank you. All right, everyone, that's our show for today. To learn more about today's guest or the topics mentioned in this interview, visit twimlai.com. Of course, if you like what you hear on the podcast, please subscribe, rate, and review the show on your favorite podcatcher. Thanks so much for listening, and catch you next time."}, "podcast_summary": "In this podcast episode, Sam Charrington interviews Robert Ness, a senior researcher at Microsoft Research, about the topic of causality and large language models (LLMs). Robert discusses a paper he co-authored on causal reasoning and LLMs, highlighting the challenges and opportunities of using LLMs for causal analysis. He explains that LLMs have shown promising results in tasks such as pairwise causal discovery and full graph causal discovery benchmarks. However, there are still limitations and uncertainties in the model's reasoning process. Robert suggests that LLMs can be used as tools to complement human causality researchers and analysts, but it's important to be aware of their limitations and potential biases. He also discusses the potential for future research in using reinforcement learning to train LLMs in causal reasoning and the use of tools like Guidance to control and constrain LLMs' outputs.", "podcast_guest": {"name": "Robert Ness", "summary": "Guest summary not found"}, "podcast_highlights": "Highlights of the podcast episode include:\n\n- Discussion about the recent paper \"Causal Reasoning and Large Language Models, Opening a New Frontier for Causality\"\n- Exploration of the field of causal analysis and its importance in various domains\n- Evaluation of large language models (LLMs) in terms of their ability to reason causally and make correct causal conclusions\n- Examination of LLMs' performance on different causal analysis benchmarks\n- Consideration of limitations and potential future developments in using LLMs for causal reasoning"}